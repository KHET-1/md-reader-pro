name: ðŸš€ Deploy to GitHub Pages

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WORKFLOW LOGGING BEST PRACTICES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# This workflow demonstrates comprehensive logging and error handling:
#
# 1. LOG CAPTURE: All critical steps use `tee` to capture output to log files
#    - Logs are written to a `logs/` directory created in each job
#    - Both stdout and stderr are captured using `2>&1 | tee -a`
#    - Exit codes are preserved using ${PIPESTATUS[0]} for proper error handling
#
# 2. ARTIFACT UPLOADS: Logs are uploaded as artifacts for debugging
#    - Individual log files for each step (e.g., lint.log, test.log)
#    - Combined logs artifact for easy download of all logs
#    - Summary logs for quick overview of step outcomes
#    - Retention period of 30 days for debugging historical failures
#
# 3. ERROR HANDLING: Steps use continue-on-error for better diagnostics
#    - Critical steps have `continue-on-error: true` to capture logs on failure
#    - Exit codes are explicitly checked and errors are logged
#    - Upload steps use `if: always()` to ensure logs are uploaded even on failure
#
# 4. STEP OUTCOMES: Job summaries track all step results
#    - Each step has an `id` for outcome tracking
#    - Summary logs include all step outcomes for quick diagnosis
#    - Step outcomes can be: success, failure, cancelled, or skipped
#
# 5. DEBUGGING WORKFLOW:
#    - Check the "Artifacts" section of a workflow run to download logs
#    - Individual logs for specific steps (e.g., "lint-logs", "test-logs")
#    - "all-*-logs" artifacts contain all logs from that job
#    - Summary logs provide quick overview without downloading everything
#
# 6. LOG FORMAT: Each log includes:
#    - Start message with emoji indicator
#    - Full command output (stdout and stderr)
#    - Exit code on failure
#    - Success/failure message at end
#
# BENEFITS:
# - Faster debugging: Logs available immediately without re-running workflow
# - Historical analysis: Logs retained for 30 days to track issues over time
# - Selective download: Download only the logs you need for specific failures
# - Better visibility: Summary logs show all step outcomes at a glance
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

on:
  push:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * *'

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  # Quality Gates Job
  quality-gates:
    name: ðŸ§ª Quality Gates
    runs-on: ubuntu-latest

    steps:
    - name: ðŸ“¥ Checkout repository (Quality Gates)
      uses: actions/checkout@v6

    - name: ðŸ“¦ Setup Node.js (Quality Gates)
      uses: actions/setup-node@v6
      with:
        node-version: '20'
        cache: 'npm'

    # Create logs directory for capturing step outputs
    - name: ðŸ“ Create logs directory
      run: mkdir -p logs

    - name: ðŸ“‹ Install dependencies (Quality Gates)
      run: npm ci

    # BEST PRACTICE: Wrap critical steps with tee to capture output to log files
    # This allows debugging even when steps fail
    - name: ðŸ” Run ESLint
      id: lint
      continue-on-error: true
      run: |
        echo "ðŸ” Starting ESLint..." | tee logs/lint.log
        npm run lint 2>&1 | tee -a logs/lint.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ ESLint failed with exit code $EXIT_CODE" | tee -a logs/lint.log
          exit $EXIT_CODE
        fi
        echo "âœ… ESLint completed successfully" | tee -a logs/lint.log

    # Upload lint logs as artifact (available even on failure)
    - name: ðŸ“¤ Upload lint logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: lint-logs
        path: logs/lint.log
        retention-days: 30

    - name: ðŸ§ª Run test suite
      id: test
      continue-on-error: true
      run: |
        echo "ðŸ§ª Starting test suite..." | tee logs/test.log
        npm run test 2>&1 | tee -a logs/test.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Tests failed with exit code $EXIT_CODE" | tee -a logs/test.log
          exit $EXIT_CODE
        fi
        echo "âœ… Tests completed successfully" | tee -a logs/test.log

    - name: ðŸ“¤ Upload test logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: test-logs
        path: logs/test.log
        retention-days: 30

    - name: ðŸ“Š Run test coverage
      id: coverage
      continue-on-error: true
      run: |
        echo "ðŸ“Š Starting test coverage..." | tee logs/coverage.log
        npm run test:coverage 2>&1 | tee -a logs/coverage.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Coverage check failed with exit code $EXIT_CODE" | tee -a logs/coverage.log
          exit $EXIT_CODE
        fi
        echo "âœ… Coverage check completed successfully" | tee -a logs/coverage.log

    - name: ðŸ“¤ Upload coverage logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: coverage-logs
        path: logs/coverage.log
        retention-days: 30

    - name: âš¡ Run performance tests
      id: performance
      continue-on-error: true
      run: |
        echo "âš¡ Starting performance tests..." | tee logs/performance.log
        npm run test:performance 2>&1 | tee -a logs/performance.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Performance tests failed with exit code $EXIT_CODE" | tee -a logs/performance.log
          exit $EXIT_CODE
        fi
        echo "âœ… Performance tests completed successfully" | tee -a logs/performance.log

    - name: ðŸ“¤ Upload performance test logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: performance-logs
        path: logs/performance.log
        retention-days: 30

    - name: ðŸ“ˆ Run benchmark tests
      id: benchmarks
      continue-on-error: true
      run: |
        echo "ðŸ“ˆ Starting benchmark tests..." | tee logs/benchmarks.log
        npm run test:benchmarks 2>&1 | tee -a logs/benchmarks.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Benchmark tests failed with exit code $EXIT_CODE" | tee -a logs/benchmarks.log
          exit $EXIT_CODE
        fi
        echo "âœ… Benchmark tests completed successfully" | tee -a logs/benchmarks.log

    - name: ðŸ“¤ Upload benchmark logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: benchmark-logs
        path: logs/benchmarks.log
        retention-days: 30

    - name: ðŸ”„ Check for performance regressions
      id: regression
      continue-on-error: true
      run: |
        echo "ðŸ”„ Starting performance regression check..." | tee logs/regression.log
        npm run performance:regression 2>&1 | tee -a logs/regression.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Performance regression check failed with exit code $EXIT_CODE" | tee -a logs/regression.log
          exit $EXIT_CODE
        fi
        echo "âœ… Performance regression check completed successfully" | tee -a logs/regression.log

    - name: ðŸ“¤ Upload regression check logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: regression-logs
        path: logs/regression.log
        retention-days: 30

    - name: ðŸ—ï¸ Test production build
      id: build-test
      continue-on-error: true
      run: |
        echo "ðŸ—ï¸ Starting production build test..." | tee logs/build-test.log
        npm run build 2>&1 | tee -a logs/build-test.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Production build test failed with exit code $EXIT_CODE" | tee -a logs/build-test.log
          exit $EXIT_CODE
        fi
        echo "âœ… Production build test completed successfully" | tee -a logs/build-test.log

    - name: ðŸ“¤ Upload build test logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: build-test-logs
        path: logs/build-test.log
        retention-days: 30

    # BEST PRACTICE: Create a summary log with all step results
    # This provides a quick overview of what passed/failed
    - name: ðŸ“‹ Create summary log
      if: always()
      run: |
        echo "=== Quality Gates Summary ===" > logs/summary.log
        echo "Generated at: $(date -u)" >> logs/summary.log
        echo "" >> logs/summary.log
        echo "Step Results:" >> logs/summary.log
        echo "- Lint: ${{ steps.lint.outcome }}" >> logs/summary.log
        echo "- Tests: ${{ steps.test.outcome }}" >> logs/summary.log
        echo "- Coverage: ${{ steps.coverage.outcome }}" >> logs/summary.log
        echo "- Performance: ${{ steps.performance.outcome }}" >> logs/summary.log
        echo "- Benchmarks: ${{ steps.benchmarks.outcome }}" >> logs/summary.log
        echo "- Regression: ${{ steps.regression.outcome }}" >> logs/summary.log
        echo "- Build Test: ${{ steps.build-test.outcome }}" >> logs/summary.log
        cat logs/summary.log || echo "Error reading logs/summary.log"

    - name: ðŸ“¤ Upload summary log
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: quality-gates-summary
        path: logs/summary.log
        retention-days: 30

    # BEST PRACTICE: Upload all logs as a single artifact for easy download
    - name: ðŸ“¦ Upload all quality gate logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: all-quality-gate-logs
        path: logs/
        retention-days: 30

  # Build and Deploy Job
  deploy:
    name: ðŸš€ Build and Deploy
    runs-on: ubuntu-latest
    needs: quality-gates
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
    - name: ðŸ“¥ Checkout repository (Deploy)
      uses: actions/checkout@v6

    - name: ðŸ“¦ Setup Node.js (Deploy)
      uses: actions/setup-node@v6
      with:
        node-version: '20'
        cache: 'npm'

    # Create logs directory for deployment logs
    - name: ðŸ“ Create logs directory
      run: mkdir -p logs

    - name: ðŸ“‹ Install dependencies (Deploy)
      run: npm ci

    # BEST PRACTICE: Capture production build output for debugging
    # Build failures often require detailed logs to diagnose
    - name: ðŸ—ï¸ Build for production
      id: build
      continue-on-error: true
      run: |
        echo "ðŸ—ï¸ Starting production build..." | tee logs/build.log
        npm run build 2>&1 | tee -a logs/build.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Production build failed with exit code $EXIT_CODE" | tee -a logs/build.log
          exit $EXIT_CODE
        fi
        echo "âœ… Production build completed successfully" | tee -a logs/build.log

    - name: ðŸ“¤ Upload build logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: production-build-logs
        path: logs/build.log
        retention-days: 30

    - name: ðŸ“ Prepare deployment assets
      id: deploy-prepare
      continue-on-error: true
      run: |
        echo "ðŸ“ Preparing deployment assets..." | tee logs/deploy-prepare.log
        npm run deploy:prepare 2>&1 | tee -a logs/deploy-prepare.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Deployment preparation failed with exit code $EXIT_CODE" | tee -a logs/deploy-prepare.log
          exit $EXIT_CODE
        fi
        echo "âœ… Deployment preparation completed successfully" | tee -a logs/deploy-prepare.log

    - name: ðŸ“¤ Upload deployment preparation logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: deploy-prepare-logs
        path: logs/deploy-prepare.log
        retention-days: 30

    - name: ðŸ“„ Setup GitHub Pages
      uses: actions/configure-pages@v5

    - name: ðŸ“¤ Upload Pages artifact
      uses: actions/upload-pages-artifact@v4
      with:
        path: './dist'

    - name: ðŸš€ Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: â„¹ï¸ GitHub Pages Setup Help
      if: failure() && steps.deployment.outcome == 'failure'
      run: |
        echo "::error title=GitHub Pages Deployment Failed::Please ensure GitHub Pages is enabled in repository settings"
        echo "::notice title=Setup Instructions::1. Go to https://github.com/${{ github.repository }}/settings/pages"
        echo "::notice::2. Under 'Build and deployment' â†’ 'Source', select 'GitHub Actions'"
        echo "::notice::3. Click 'Save' and re-run this workflow"
        echo "::notice::For detailed instructions, see DEPLOYMENT.md in the repository"

    # BEST PRACTICE: Create deployment summary with all step outcomes
    - name: ðŸ“‹ Create deployment summary log
      if: always()
      run: |
        echo "=== Deployment Summary ===" > logs/deploy-summary.log
        echo "Generated at: $(date -u)" >> logs/deploy-summary.log
        echo "" >> logs/deploy-summary.log
        echo "Step Results:" >> logs/deploy-summary.log
        echo "- Build: ${{ steps.build.outcome }}" >> logs/deploy-summary.log
        echo "- Deploy Prepare: ${{ steps.deploy-prepare.outcome }}" >> logs/deploy-summary.log
        echo "- Deployment: ${{ steps.deployment.outcome }}" >> logs/deploy-summary.log
        echo "" >> logs/deploy-summary.log
        if [ "${{ steps.deployment.outcome }}" == "success" ]; then
          echo "Deployment URL: ${{ steps.deployment.outputs.page_url }}" >> logs/deploy-summary.log
        fi
        cat logs/deploy-summary.log || echo "Error reading logs/deploy-summary.log"

    - name: ðŸ“¤ Upload deployment summary
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: deployment-summary
        path: logs/deploy-summary.log
        retention-days: 30

    - name: ðŸ“¦ Upload all deployment logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: all-deployment-logs
        path: logs/
        retention-days: 30

    - name: ðŸ“Š Deployment summary
      run: |
        echo "## ðŸŽ‰ Deployment Successful!" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**ðŸ“± Application URL**: ${{ steps.deployment.outputs.page_url }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### âœ… Quality Gates Passed" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ” **ESLint**: No errors" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ§ª **Tests**: All passing" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“Š **Coverage**: 94.7% with E2E validation" >> $GITHUB_STEP_SUMMARY
        echo "- âš¡ **Performance**: All benchmarks green" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ—ï¸ **Build**: Production ready" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“Š Deployment Details" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- **Time**: $(date -u)" >> $GITHUB_STEP_SUMMARY
        echo "- **Workflow**: [\#${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“¦ Artifacts Available" >> $GITHUB_STEP_SUMMARY
        echo "- Production build logs" >> $GITHUB_STEP_SUMMARY
        echo "- Deployment preparation logs" >> $GITHUB_STEP_SUMMARY
        echo "- All deployment logs (combined)" >> $GITHUB_STEP_SUMMARY

  # Performance Monitoring Job (runs on schedule)
  performance-monitor:
    name: ðŸ“ˆ Performance Monitor
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
    - name: ðŸ“¥ Checkout repository (Monitor)
      uses: actions/checkout@v6

    - name: ðŸ“¦ Setup Node.js (Monitor)
      uses: actions/setup-node@v6
      with:
        node-version: '20'
        cache: 'npm'

    # Create logs directory for monitoring logs
    - name: ðŸ“ Create logs directory
      run: mkdir -p logs

    - name: ðŸ“‹ Install dependencies (Monitor)
      run: npm ci

    # BEST PRACTICE: Monitor performance trends with detailed logging
    # Helps identify performance degradation over time
    - name: ðŸ“Š Run performance monitoring
      id: monitor
      continue-on-error: true
      run: |
        echo "ðŸ“Š Starting performance monitoring..." | tee logs/monitor.log
        npm run performance:monitor 2>&1 | tee -a logs/monitor.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Performance monitoring failed with exit code $EXIT_CODE" | tee -a logs/monitor.log
          exit $EXIT_CODE
        fi
        echo "âœ… Performance monitoring completed successfully" | tee -a logs/monitor.log

    - name: ðŸ“¤ Upload performance monitoring logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: performance-monitor-logs
        path: logs/monitor.log
        retention-days: 30

    - name: ðŸ”„ Check for regressions
      id: regression-check
      continue-on-error: true
      run: |
        echo "ðŸ”„ Starting regression check..." | tee logs/regression-check.log
        npm run performance:regression 2>&1 | tee -a logs/regression-check.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Regression check failed with exit code $EXIT_CODE" | tee -a logs/regression-check.log
          exit $EXIT_CODE
        fi
        echo "âœ… Regression check completed successfully" | tee -a logs/regression-check.log

    - name: ðŸ“¤ Upload regression check logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: performance-regression-logs
        path: logs/regression-check.log
        retention-days: 30

    - name: ðŸ“ˆ Update performance baselines
      id: baseline-update
      continue-on-error: true
      run: |
        echo "ðŸ“ˆ Updating performance baselines..." | tee logs/baseline-update.log
        npm run performance:update-baseline 2>&1 | tee -a logs/baseline-update.log
        EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXIT_CODE -ne 0 ]; then
          echo "âŒ Baseline update failed with exit code $EXIT_CODE" | tee -a logs/baseline-update.log
          exit $EXIT_CODE
        fi
        echo "âœ… Baseline update completed successfully" | tee -a logs/baseline-update.log

    - name: ðŸ“¤ Upload baseline update logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: baseline-update-logs
        path: logs/baseline-update.log
        retention-days: 30

    # BEST PRACTICE: Create monitoring summary for quick insights
    - name: ðŸ“‹ Create monitoring summary
      if: always()
      run: |
        echo "=== Performance Monitoring Summary ===" > logs/monitor-summary.log
        echo "Generated at: $(date -u)" >> logs/monitor-summary.log
        echo "" >> logs/monitor-summary.log
        echo "Step Results:" >> logs/monitor-summary.log
        echo "- Performance Monitor: ${{ steps.monitor.outcome }}" >> logs/monitor-summary.log
        echo "- Regression Check: ${{ steps.regression-check.outcome }}" >> logs/monitor-summary.log
        echo "- Baseline Update: ${{ steps.baseline-update.outcome }}" >> logs/monitor-summary.log
        cat logs/monitor-summary.log || echo "Error reading logs/monitor-summary.log"

    - name: ðŸ“¤ Upload monitoring summary
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: performance-monitoring-summary
        path: logs/monitor-summary.log
        retention-days: 30

    - name: ðŸ“¦ Upload all monitoring logs
      if: always()
      uses: actions/upload-artifact@v5
      with:
        name: all-performance-monitor-logs
        path: logs/
        retention-days: 30